[
  {
    "id": "a_better_way_to_address_cecl",
    "image": "pexels-support.jpg",
    "title": "A Better Way to Address CECL",
    "categories": "CECL / Modeling",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "Christos A. Makridis holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, Institute for Studies of Religion, as well serving as an adjunct fellow at the Manhattan Institute and senior adviser at Gallup. Christos previously served on the White House Council of Economic Advisers managing the cybersecurity, technology, and space activities, as a Non-resident Fellow at the Cyber Security Project in the Harvard Kennedy School of Government, as a Digital Fellow at the Initiative at the Digital Economy in the MIT Sloan School of Management. Christos' primary academic research focuses on labor economics, the digital economy, and personal finance and well-being, and has published over 70 peer-reviewed research articles and 170 news articles. He serves on the board of advisers for Meta Impact Capital and Zenger News. Christos earned dual Masters and PhDs in Economics and Management Science & Engineering at Stanford University.",
    "published": "",
    "excerpt": "The FASB issued the new CECL accounting standard in June 2016 for estimating allowances for credit losses. What does this mean for your bank?",
    "text": "<p>The Financial Accounting Standards Board (FASB) issued a new accounting standard in June 2016, introducing the current expected credit losses methodology (CECL) for estimating allowances for credit losses. While most banks were required to comply following December 15, 2019, smaller and mid sized banks only need to begin following it after December 15, 2022.</p><p><strong>What does this mean?</strong></p><p>Unlike the previous accounting that allowed banks to report the value of an asset, like a loan, at the time it was originated (“incurred loss&quot;), the new methodology requires banks to have a forecast over their current balance sheet to determine the amount of reserves to have on hand.</p><p>Forecasting, however, is tough, and even U.S. regulatory agencies realize that no one is going to get it right. But that&apos;s also not an excuse to completely miss the mark: the responsibility is on banks to do their due diligence and prepare credible forecasts. The specific language that the Office of Comptroller of Currency (OCC) uses is “reasonable and supportable.&quot; While it&apos;s not enough to simply look at historical data, banks can, of course, use historical data to inform their forecast.</p><p>Let&apos;s turn to their specific language</p><blockquote>326-20-30-9: An entity shall not rely solely on past events to estimate expected credit losses. When an entity uses historical loss information, it shall consider the need to adjust historical information to reflect the extent to which management expects current conditions and reasonable and supportable forecasts to differ from the conditions that existed for the period over which historical information was evaluated. The adjustments to historical loss information may be qualitative in nature and should reflect changes related to relevant data (such as changes in unemployment rates, property values, commodity values, delinquency, or other factors that are associated with credit losses on the financial asset or in the group of financial assets). Some entities may be able to develop reasonable and supportable forecasts over the contractual term of the financial asset or a group of financial assets. However, an entity is not required to develop forecasts over the contractual term of the financial asset or group of financial assets. Rather, for periods beyond which the entity is able to make or obtain reasonable and supportable forecasts of expected credit losses, an entity shall revert to historical loss information determined in accordance with paragraph 326-20-30-8 that is reflective of the contractual term of the financial asset or group of financial assets. An entity shall not adjust historical loss information for existing economic conditions or expectations of future economic conditions for periods that are beyond the reasonable and supportable period. An entity may revert to historical loss information at the input level or based on the entire estimate. An entity may revert to historical loss information immediately, on a straight-line basis, or using another rational and systematic basis.</blockquote><p>That paragraph has three main points:</p><ul><li>Estimates of loan reserves based on historical data alone is insufficient</li><li>Qualitative adjustments to historical data can be based on judgment, but that judgment should be anchored in some way in relevant data</li><li>Entities can, but are not required to, develop reasonable and supportable forecasts over the contractual term of an asset, and in those cases should revert to credible historical estimates</li></ul><p><strong>Existing approaches</strong></p><p>But that begs the question, what data is relevant for creating a forecast? As far as we know, the existing consulting or software packages fall into one of two categories:</p><ol><li>Forecast economic conditions using national data</li><li>Produce multiple scenarios of high, medium or low forecasts based on qualitative factors</li></ol><p>Each approach has substantial limitations. Approach #1 takes a one-size-fits-all approach using 1980s econometric methods, rather than state-of-the-art machine learning models that can accommodate big data at more disaggregated levels, such as a zipcode or a county. Given that small and medium sized banks are inherently local and specialize in niche areas, a one-size-fits-all approach may force banks to hold out more reserves than they really need based on their own risk profile.</p><p>Setting aside the importance of regional heterogeneity and the specific ecosystem that each bank operates in, off-the-shelf macroeconometric tools are highly inaccurate. These packages do not follow best practices from computer science where, for example, the modeler holds out a subset of years and uses the model to produce an out-of-sample forecast that is compared with the data that was held out. Instead, the models are fit over the entire data, producing in-sample fits that are overly optimistic. Moreover, these forecasts can change on a whim; organizations routinely update forecasts to “take into account updated economic conditions&quot; without conceding that their earlier forecast was off. Simply put, forecasting today is highly inaccurate and may give the impression of security because of all the equations, but in reality they fail to reflect the dynamic features of the data.</p><p>Approach #2, while much simpler, also has its own challenges, namely the arbitrary nature of the high, medium, and low forecasts. What is to say 3% gross domestic product growth is high and 1% is low? A recent vein of economics research has pointed out that productivity has slowed over the past two decades, with some economists calling it an era of “secular stagnation,&quot; so does that mean that 1% GDP growth is high? The problem here is that the thresholds are arbitrary and do not take advantage of the availability of vast amounts of data that can be used to discipline the forecast.</p><p>Dissatisfaction with current forecasting methods abounds. While the large banks have their own teams to handle bank-specific forecasts, which often make the news when these forecasts are released, smaller and mid sized banks do not have the budget to hire teams of data scientists and economists. Michael Gullette, senior vice president, tax and accounting at the American Bankers Association (ABA), has commented that ABA roundtables have revealed substantial concern among these smaller banks about how to make adjustments to their forecasts based on emerging economic events. “The big banks had a grip on that,&quot; Gullette said. “They&apos;ve understood progressively over the last few years that that is where we&apos;re headed, but the smaller banks [under $30 billion in assets] not so much. They&apos;re saying, &apos;How do we support these adjustments?&apos;&quot;</p><p><strong>Our data-driven approach</strong></p><p>We believe that sophisticated forecasting tools should be available to everyone, much like the consumer product and service industry (e.g., internet and iPhones). Using data from a wide array of seemingly disparate sources, ranging from the Census Bureau to Bureau of Labor Statistics, we build highly reliable predictive models around each asset class. Crucially, we include local information, rather than just national averages, when computing forecasts for an area and asset class. </p><p>Further, we can explicitly show our degree of accuracy using standard methods from computer science, often referred to as cross-validation. That way, we can show exactly how accurate our models are and quickly identify where there are gaps so that they can be rectified and caveated.</p><p>Once we obtain predictions for each asset class, we now have its contemporaneous value and a forecast. We run that forecast forward for however many years are on the contractual term of the asset, whether it&apos;s a 30 year fixed rate mortgage or a 5-1 adjustable rate mortgage. We also obtain a confidence interval over these estimates so that we have a band of uncertainty.</p><p>Then, the decision over how many reserves to hold is up to the bank: how many reserves do you need so that you do not fall below a particularly worrisome threshold?</p><p>Join us on the journey.</p>"
  },
  {
    "id": "cecl_regulatory_compliance_made_easy",
    "image": "pexels-success.jpg",
    "title": "CECL Regulatory Compliance Made Easy",
    "categories": "CECL",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "Christos A. Makridis holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, Institute for Studies of Religion, as well serving as an adjunct fellow at the Manhattan Institute and senior adviser at Gallup. Christos previously served on the White House Council of Economic Advisers managing the cybersecurity, technology, and space activities, as a Non-resident Fellow at the Cyber Security Project in the Harvard Kennedy School of Government, as a Digital Fellow at the Initiative at the Digital Economy in the MIT Sloan School of Management. Christos' primary academic research focuses on labor economics, the digital economy, and personal finance and well-being, and has published over 70 peer-reviewed research articles and 170 news articles. He serves on the board of advisers for Meta Impact Capital and Zenger News. Christos earned dual Masters and PhDs in Economics and Management Science & Engineering at Stanford University.",
    "link": "#",
    "published": "",
    "excerpt": "Most data models are in poor shape, making compliance hard. Dainamic's models can make it easy.",
    "text": "<p>The Current and Expected Credit Losses (CECL) standard was introduced by the Financial Accounting Standards Board a few years ago, and all banks must transition by December 15, 2022. CECL requires &quot;reasonable and supportable forecasts,&quot; which requires not only historical loan data, but also external data and a model that can be used to adjust for changes in economic conditions and a range of uncertainty over each scenario.</p><p>Sadly, most of the available models are dreadful and banks resort to historical averages of loan losses and aggregate conditions. This makes complying with CECL challenging for small and medium sized financial institutions for a number of reasons.</p><p><strong>1. Data consolidation takes time </strong></p><p>There is far too much data available for any single person to track, and there are no real platforms that quickly aggregate this data for an individual to use for better decisions.</p><p><strong>2. Signals vs noise</strong></p><p>It is difficult to get the right market signals from the data. Once you have collected the data it will take even more time to organize it and make sense of it all for actionable intelligence for decision-making.</p><p><strong>3. The pace of change renders even recent forecasts obsolete </strong></p><p>Forecasts from two months ago are not reliable today, especially in an era with substantial economic, social, and geopolitical uncertainty.</p><p><strong>4. Quantitative skills are expensive</strong></p><p>Hiring a competent team of data scientists is expensive and generally outside of the budget for mid/small sized banks. And because the new regulations are increasing the demand for data scientists, an area of expertise that is very in-demand in other industries, it will make it all the more difficult for small firms to build such teams. </p><p><strong>Dainamic can help </strong></p><p>The goal of Dainamic is to help banks with these key issues. First, we are a well qualified team of data scientists and economists who can take on the difficult task of data collection. We use the most sophisticated machine learning methods available with granular and comprehensive data over time to produce accurate and interpretable forecasts.</p><p>We also have the ability to analyze and visualize the data which will make it much easier for financial institutions to report it to regulators and other key stakeholders. This ultimately helps banks to make more reliable forecasts which will result in less stringent capital requirements and will help them deploy capital in more strategic ways.</p><p>To put it simply, better data drives better models. Better models drive better forecasts. And better forecasts drive better planning so banks can invest more intelligently. That is the goal of Dainamic. </p> <p>If you would like to learn more or would like our help, please contact us.</p>"
  },
  {
    "id": "modeling_capital_reserves_under_cecl",
    "image": "pexels-financial-model.jpg",
    "title": "Modeling capital reserves under CECL",
    "categories": "CECL",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "Christos A. Makridis holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, Institute for Studies of Religion, as well serving as an adjunct fellow at the Manhattan Institute and senior adviser at Gallup. Christos previously served on the White House Council of Economic Advisers managing the cybersecurity, technology, and space activities, as a Non-resident Fellow at the Cyber Security Project in the Harvard Kennedy School of Government, as a Digital Fellow at the Initiative at the Digital Economy in the MIT Sloan School of Management. Christos' primary academic research focuses on labor economics, the digital economy, and personal finance and well-being, and has published over 70 peer-reviewed research articles and 170 news articles. He serves on the board of advisers for Meta Impact Capital and Zenger News. Christos earned dual Masters and PhDs in Economics and Management Science & Engineering at Stanford University.",
    "published": "",
    "excerpt": "Once you have a reasonable and supportable forecast, how do you determine the amount of capital reserves that you actually need?",
    "text": "<p>The current expected credit losses methodology (CECL) for estimating allowances for credit losses requires that banks develop reasonable and supportable forecasts, which we&apos;ve written about before here. But once you have a reasonable and supportable forecast, how do you actually determine the amount of capital reserves that you need?</p><p>Conceptually, when forecasts suggest a downturn, a bank needs to have more reserves on hand. The rationale is simple: in a downturn, consumers and businesses are more likely to be delinquent on their loans, which not only reduces cash flow for a bank, but also increases the odds of foreclosure on the property, thereby affecting the value of their balance sheet and financial stability. In this sense, having more reserves on hand provides an extra cushion in the event of a crisis.</p><p>But holding reserves is costly. Sticking cash under a mattress has an opportunity cost – banks cannot loan that money out to provide investment and consumption opportunities to consumers and businesses, in turn generating revenue on the interest on those loans. </p><p>We need a gold standard approach to determining reserves, striking the right balance between mitigating any risk of bank insolvency and providing value-generating opportunities.</p><p>We believe that modeling capital reserves is not rocket science – and frankly there&apos;s probably more art than science in the modeling exercise since economics is fundamentally about behavior. Fancy models are helpful, but they cannot eliminate uncertainty all together. Here&apos;s our approach:</p><ol><li>Generate bank or region specific forecasts – Depending on the size of the bank, you may simply need to know what&apos;s happening to a state or metropolitan area, but the key is that you obtain local predictions, rather than national ones. Even during the financial crisis of 2007-08, there were still some counties that were doing well, so especially for small and mid sized banks that do not have national exposure, there is no sense in using national forecasts.</li><li>Obtain a confidence interval on the forecast – The quality and availability of data allows us to produce forecasts that have some degree of precision. Then, you want to choose how much precision you are willing to accept – for example, a 99% confidence interval might be more than enough to rule out negative outcomes, particularly in light with other risks that the bank might take on a more regular basis.</li><li>Toggle the amount of reserves by asset class to achieve the desired level of safety – Since each asset class is differentially exposed to risk because of either the composition of buyers in the market or other complementary asset classes it is linked to, banks can play around with different choices of reserves across the asset classes they are invested in to achieve their desired level of safety and security.</li></ol><p>These are not easy to do overnight for an organization, but we have made the tools available on our platform so that you do not need to go through the painstaking process of building it from scratch.</p><p>Learn more about the Dainamic Dashboard.</p>"
  }
]
