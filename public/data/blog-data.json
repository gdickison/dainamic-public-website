[
  {
    "id": "a_better_way_to_address_cecl",
    "hidden": false,
    "image": "pexels-support.jpg",
    "title": "How Small and Midsized Banks Should Address CECL",
    "categories": "CECL / Modeling",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "<p><a href='/about'>Dr. Christos A. Makridis</a> is the founder and CEO of Dainamic and holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, and a senior adviser at Gallup.</p>",
    "published": "",
    "excerpt": "The FASB issued the new CECL accounting standard in June 2016 for estimating allowances for credit losses. What does this mean for your bank?",
    "text": "<h1 class='text-5xl'>How Small and Midsized Banks Should Address CECL</h1><p>The <a href='https://www.fasb.org/'>Financial Accounting Standards Board</a> (FASB) issued a new accounting standard in June 2016, introducing the <a href='https://www.occ.treas.gov/topics/supervision-and-examination/bank-operations/accounting/current-expected-credit-losses/index-current-expected-credit-losses.html'>current expected credit losses methodology</a> (CECL) for estimating allowances for credit losses. While most banks were required to comply following December 15, 2019, smaller and mid sized banks only need to begin following it after December 15, 2022.</p><h2 class='text-3xl'>What does CECL require banks to do?</h2><p>Unlike the previous accounting standards that allowed banks to report the value of an asset, like a loan, at the time it was originated (&quot;incurred loss&quot;), CECL requires banks to have a forecast over their current balance sheet to determine the amount of reserves to have on hand.</p><p>Forecasting, however, is tough, and even U.S. regulatory agencies realize that no one is going to get it right. But that's also not an excuse to completely miss the mark: the responsibility is on banks to do their due diligence and prepare credible forecasts. The specific language that the <a href='https://www.occ.treas.gov/'>Office of Comptroller of Currency</a> (OCC) uses is &quot;reasonable and supportable.&quot;</p><p>Historical data will play an integral role in producing reasonable and supportable forecasts, but how one leverages historical data, ranging from the type of data to the statistical model, heavily influences whether the forecasts are actually reliable and practically useful.</p><h2 class='text-3xl'>Unpacking CECL</h2><p>As a result, we need to dig in a bit more to understand the new requirements.</p><blockquote>326-20-30-9: An entity shall not rely solely on past events to estimate expected credit losses. When an entity uses historical loss information, it shall consider the need to adjust historical information to reflect the extent to which management expects current conditions and reasonable and supportable forecasts to differ from the conditions that existed for the period over which historical information was evaluated. The adjustments to historical loss information may be qualitative in nature and should reflect changes related to relevant data (such as changes in unemployment rates, property values, commodity values, delinquency, or other factors that are associated with credit losses on the financial asset or in the group of financial assets). Some entities may be able to develop reasonable and supportable forecasts over the contractual term of the financial asset or a group of financial assets. However, an entity is not required to develop forecasts over the contractual term of the financial asset or group of financial assets. Rather, for periods beyond which the entity is able to make or obtain reasonable and supportable forecasts of expected credit losses, an entity shall revert to historical loss information determined in accordance with paragraph 326-20-30-8 that is reflective of the contractual term of the financial asset or group of financial assets. An entity shall not adjust historical loss information for existing economic conditions or expectations of future economic conditions for periods that are beyond the reasonable and supportable period. An entity may revert to historical loss information at the input level or based on the entire estimate. An entity may revert to historical loss information immediately, on a straight-line basis, or using another rational and systematic basis.</blockquote><h2 class='text-3xl'>Here are the three main points</h2><p><strong>1. Estimates of loan reserves based on historical data alone is insufficient</strong></p><p>Standard forecasts combine historical data points together and predict future values based on linear trends with an autoregressive component (i.e., the value in the previous time period). But these forecasts fail to account for the non-linearities and structure inherent in the data. For example, for most small and mid sized banks, national trends are not informative for the loan reserves that they will need based on their regional exposure â€“ that is, what happens in Nashville, TN can be quite different than what happens in the nation as a whole.</p><p>By relying on aggregate data and simple statistical models that abstract from realistic interactions among variables of interest and local information, forecasts are unreliable at best and misleading at worst. In short, historical data can and should be used, but it must be layered by a sound data-driven infrastructure and theory to harness its full capabilities.</p><p><strong>2. Qualitative adjustments to historical data can be based on judgment, but that judgment should be anchored in some way in relevant data</strong></p><p>Good or bad, there are many phenomena that are not easy to convert into comparable categorical or continuous values for predictive modeling. While that should not stop us from paying attention to such qualitative factors, they need to be integrated into quantitative models in the right way to have a credible interpretation. Fortunately, there are almost always empirical counterparts to qualitative insights, which can be leveraged to efficiently integrate the qualitative factors into the model in a way that is scalable, transparent, and reliable.</p><p><strong>3. Entities can, but are not required to, develop reasonable and supportable forecasts over the contractual term of an asset, and in those cases should revert to credible historical estimates</strong></p><p>Since many loans originated with an anticipated long life cycle, it is impossible to predict the expected losses in these categories without the use of historical data. For example, forecasting the number of mortgages originated in 2020 with a 30 year fixed rate loan that will go delinquent is a loaded question: the loan could go delinquent at any point in the next 20 years, so the only way to tractably model these expected losses is to rely on the life cycle of comparable loans that were originated in the past and compare apples-to-apples as best as possible.</p><h2 class='text-3xl'>What data is relevant for creating a forecast?</h2><p>But that begs the question, what data is relevant for creating a forecast? As far as we know, the existing consulting or software packages fall into one of two categories:</p><ol><li><strong>1. Forecast economic conditions using national data</strong></li><li><strong>2. Produce multiple scenarios of high, medium or low forecasts based on qualitative factors</strong></li></ol><p>Each approach has substantial limitations.</p><p><strong>Approach #1 takes a one-size-fits-all approach using 1980s econometric methods</strong>, rather than state-of-the-art machine learning models that can accommodate big data at more disaggregated levels, such as a ZIP code or a county. Given that small and medium sized banks are inherently local and specialize in niche areas, a one-size-fits-all approach will force banks to hold out more reserves than they really need based on their own risk profile.</p><p>Setting aside the importance of regional heterogeneity and the specific ecosystem that each bank operates in, off-the-shelf macroeconometric tools are highly inaccurate. These packages do not follow best practices from computer science where, for example, the modeler holds out a subset of years and uses the model to produce an out-of-sample forecast that is compared with the data that was held out. Instead, the models are fit over the entire data, producing in-sample fits that are overly optimistic.</p><p>Moreover, these forecasts can change on a whim; organizations routinely update forecasts to &quot;take into account updated economic conditions&quot;without conceding that their earlier forecast was off. Simply put, forecasting today is highly inaccurate and may give the impression of security because of all the equations, but in reality they fail to reflect the dynamic features of the data.</p><p><strong>Approach #2, while much simpler, also has its own challenges, namely the arbitrary nature of the high, medium, and low forecasts.</strong> What is to say 3% gross domestic product growth is high and 1% is low? A recent vein of economics research has pointed out that productivity has slowed over the past two decades, with some economists calling it an era of &quot;secular stagnation,&quot; so does that mean that 1% GDP growth is high? The problem here is that the thresholds are arbitrary and do not take advantage of the availability of vast amounts of data that can be used to discipline the forecast.</p><h2 class='text-3xl'>Dissatisfaction with current forecasting methods abounds</h2><p>While the large banks have their own teams to handle bank-specific forecasts, which often make the news when these forecasts are released, smaller and mid sized banks do not have the budget to hire teams of data scientists and economists. Michael Gullette, senior vice president, tax and accounting at the <a href='https://www.aba.com/'>American Bankers Association</a> (ABA), has commented that ABA roundtables have revealed substantial concern among these smaller banks about how to make adjustments to their forecasts based on emerging economic events. &quot;The big banks had a grip on that,&quot; Gullette <a href='https://www.garp.org/risk-intelligence/credit/with-cecl-in-force-banks-remain-cautious-on-loss-reserves'>said</a>. &quot;They've understood progressively over the last few years that that is where we're headed, but the smaller banks [under $30 billion in assets] not so much. They're saying, 'How do we support these adjustments?'&quot; </p><h2 class='text-3xl'>A data-driven approach is needed</h2><p>We believe that sophisticated forecasting tools should be available to everyone, much like the consumer product and service industry (e.g., internet and iPhones). Using data from a wide array of seemingly disparate sources, ranging from the Census Bureau to Bureau of Labor Statistics, we build highly reliable predictive models around each asset class. Crucially, we include local information, rather than just national averages, when computing forecasts for an area and asset class.</p><p>Further, we can explicitly show our degree of accuracy using standard methods from computer science, often referred to as cross-validation. That way, we can show exactly how accurate our models are and quickly identify where there are gaps so that they can be rectified and caveated.</p><p>Once we obtain predictions for each asset class, we now have its contemporaneous value and a forecast. We run that forecast forward for however many years are on the contractual term of the asset, whether it's a 30 year fixed rate mortgage or a 5-1 adjustable rate mortgage. We also obtain a confidence interval over these estimates so that we have a band of uncertainty.</p><p>Then, the decision over how many reserves to hold is up to the bank: how many reserves do you need so that you do not fall below a particularly worrisome threshold?</p><p>If you are interested in learning more, please <a href='mailto:christos@dainamic.ai'>contact us</a> or <a href='/contact'>sign up</a> to follow our research and thought leadership on forecasting, credit, risk, and regulatory compliance in financial services.</p>"
  },
  {
    "id": "ai_and_fintech_security",
    "hidden": false,
    "image": "pexels-financial-model.jpg",
    "title": "Leveraging AI to Fortify Network Security in Fintech and Blockchain Ecosystems",
    "categories": "AI / Fintech",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "<p><a href='/about'>Dr. Christos A. Makridis</a> is the founder and CEO of Dainamic and holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, and a senior adviser at Gallup.</p>",
    "published": "September 12, 2023",
    "excerpt": "Fintech platforms handle vast amounts of sensitive data ranging from personal identification information to intricate financial transactions. Traditional security measures often fall short in proactively identifying vulnerabilities and potential attacks. This is where AI steps in.",
    "text": "<h1 class='text-5xl'>Leveraging AI to Fortify Network Security in Fintech and Blockchain Ecosystems</h1><p>Fintech, and its increasing use of blockchain technology, has been two of the fastest growing sectors with an annual growth rate of 33.6% expected for 2024, according to Statista. However, with advancements come challengesâ€”especially in the domain of network security. As financial and other professional services firms increasingly deal with sensitive financial transactions and confidential data, the need for robust security mechanisms is paramount. Artificial Intelligence (AI) has the potential to revolutionize how we approach network security.</p><h2 class='text-3xl'>The AI Advantage in Fintech Security</h2><p>Fintech platforms handle vast amounts of sensitive data ranging from personal identification information to intricate financial transactions. Traditional security measures often fall short in proactively identifying vulnerabilities and potential attacks. This is where AI steps in.<p><p>AI algorithms are adept at analyzing large datasets to identify patterns and anomalies. For instance, machine learning models can be trained to detect fraudulent activities in real-time by scrutinizing transaction patterns. This proactive approach significantly reduces the time between the initiation of a threat and its detection, thereby enhancing the security of fintech platforms.</p><h2 class='text-3xl'>Blockchain Security Through AI</h2><p>Blockchain is often touted as a secure technology due to its decentralized nature and cryptographic algorithms. However, it's not entirely immune to security risks. Smart contracts, a cornerstone of blockchain technology, can sometimes contain vulnerabilities that attackers exploit. AI-driven tools can automatically audit smart contracts, flagging potential vulnerabilities long before they can be exploited. Natural Language Processing (NLP) and advanced machine learning algorithms to scan through lines of code, providing an added layer of security.</p><h2 class='text-3xl'>Federated Learning and Data Privacy</h2><p>Data privacy is a critical aspect of security, especially with regulations like GDPR coming into play. Federated learning, an AI technique, allows for the decentralized training of machine learning models. This means that sensitive data never has to leave its originating device, yet contributes to a global model. This is particularly beneficial in blockchain networks where data privacy is a fundamental attribute.</p><h2 class='text-3xl'>AI-Driven Risk Assessment</h2><p>Risk assessment is a crucial part of network security. AI algorithms can analyze historical data and real-time metrics to gauge the likelihood of security incidents. In fintech platforms, this could mean assessing the risk profile of loan applicants automatically. In blockchain, it could mean evaluating the risk associated with a particular smart contract or transaction.</p><h2 class='text-3xl'>Ethical and Regulatory Considerations</h2><p>Implementing AI in fintech and blockchain security comes with its own set of ethical and regulatory challenges. Data privacy and user consent are paramount. AI algorithms should be designed to be transparent and accountable to comply with existing laws and ethical norms.</p><h2 class='text-3xl'>Final Thoughts</h2><p>AI offers a robust toolkit for enhancing the security of networks, particularly in sectors like fintech and blockchain. From real-time threat detection to smart contract auditing, the applications are vast and transformative. However, it's essential to navigate the ethical and regulatory landscape carefully to fully harness the capabilities of AI in securing digital ecosystems. By integrating AI into their security frameworks, banks can not only protect themselves against a broad spectrum of threats, but also gain a competitive edge in today's digital economy.</p><p><strong>How artificial intelligence tools and techniques can be applied to enhance the security of decentralized networks, including threat detection, anomaly identification, and risk assessment</strong></p><p><u>Threat Detection</u></p><p>Real-time Analysis: Machine learning algorithms can analyze transaction patterns and network activities in real-time to detect potential threats or unauthorized activities.</p><p>Intrusion Detection Systems: AI can automate the identification of unusual patterns or potential intrusions, reducing the time between a threat's initiation and its detection.</p><p>Phishing Prevention: Natural Language Processing (NLP) can be used to scan communication within the network to identify phishing attempts or malicious links.</p><p><u>Anomaly Identification</u></p><p>Behavioral Analytics: Machine learning models can be trained to understand 'normal' behavior within the network, making it easier to spot anomalies.</p><p>Isolation Forests: This ensemble algorithm is effective for high-dimensional datasets and can identify anomalies in network behavior.</p><p>Time-Series Analysis: LSTM (Long Short-Term Memory) networks can analyze time-series data like transaction history to identify sudden, anomalous spikes or patterns.</p><p><u>Risk Assessment</u></p><p>Predictive Analytics: AI can forecast potential vulnerabilities by analyzing historical data and identifying patterns that may signify future risks.</p><p>Smart Contract Auditing: AI algorithms can scrutinize smart contract code for loopholes or vulnerabilities, assigning a risk score based on the likelihood of exploitation.</p><p>Node Trustworthiness: Machine learning algorithms can evaluate the behavior of nodes over time to assign trust scores, aiding in decentralized consensus mechanisms.</p><p><strong>What are some specific examples of how AI is being used to protect decentralized networks?</strong></p><p><u>Fraud Detection in Decentralized Finance (DeFi)</u></p><p>AI algorithms are being employed to monitor real-time transactions in DeFi platforms to detect fraudulent activities. These algorithms analyze patterns and flag anomalies, thereby automating the safeguarding of digital assets.</p><p><u>Smart Contract Analysis</u></p><p>Platforms like MythX and ChainSecurity leverage machine learning algorithms to analyze smart contract code for vulnerabilities. They can automatically identify issues such as reentrancy attacks, integer overflows, and more.</p><p><u>Decentralized Identity Verification</u></p><p>Projects like uPort and Sovrin use AI-driven biometric and behavioral analytics to offer secure, self-sovereign identity solutions. These analytics improve the accuracy and security of identity verification without central authorities.</p><p><u>Anomaly Detection in P2P Networks</u></p><p>AI-based anomaly detection systems are being used to identify malicious nodes or unauthorized activities in peer-to-peer networks. Techniques like isolation forests or neural networks are employed for this purpose.</p><p><u>Phishing Attack Prevention</u></p><p>Machine learning algorithms are being used to analyze text and URL patterns to detect phishing attempts in decentralized applications (dApps) and digital wallets.</p><p><u>Decentralized Data Marketplaces</u></p><p>Platforms like Ocean Protocol use federated learning and differential privacy to create secure, privacy-preserving data marketplaces. Here, AI algorithms help in the valuation and anonymization of data.</p><p><u>Threat Intelligence</u></p><p>AI-driven threat intelligence platforms are being integrated into blockchain networks to provide predictive insights into potential future attacks based on historical data and real-time network behavior.</p><p><u>Network Optimization</u></p><p>Reinforcement learning algorithms are being used to optimize network protocols, making them adaptive to different kinds of attacks or congestions, thereby enhancing overall security.</p><p><u>Ethical Considerations</u></p><p>Autonomy: The use of AI in decentralized networks must respect the principle of user autonomy, given that the decentralized nature is aimed at reducing central control.</p><p>Transparency: Due to the often opaque nature of AI algorithms, there needs to be mechanisms for accountability and explanation, especially in security contexts.</p><p><u>Potential Bottlenecks</u></p><p>Adversarial Attacks: AI models are susceptible to adversarial attacks, where slight modifications to the input data can lead to incorrect outputs.</p><p>Resource Constraints: AI algorithms, especially those that require real-time analysis, can be resource-intensive, which may not be feasible for all nodes in a decentralized network.</p><p>The integration of AI into decentralized networks offers promising avenues for enhanced security but comes with its own set of challenges that require nuanced approaches for effective implementation.</p><p><strong>What are some of the latest advances in AI security tools?</strong></p><p><u>Adversarial Machine Learning</u></p><p>The development of defense mechanisms against adversarial attacks has seen significant progress. Researchers are designing models that are more robust to slight perturbations in input data, which could otherwise mislead the AI.</p><p><u>Explainable AI (XAI)</u></p><p>XAI aims to make AI decision-making transparent and understandable to humans. This is crucial in security settings where understanding the rationale behind decisions can be critical for trust and accountability.</p><p><u>Automated Threat Hunting</u></p><p>AI algorithms are being used to autonomously search through network data for signs of compromise or vulnerabilities. This proactive approach enhances the speed and effectiveness of threat detection.</p><p><u>Secure and Private AI</u></p><p>Technologies like federated learning, differential privacy, and homomorphic encryption are advancing to enable more secure and private machine learning models. These are particularly useful for complying with privacy regulations like GDPR.</p><p><u>AI-Driven Risk Assessment</u></p><p>Machine learning models are increasingly used to evaluate the security posture of a system or network automatically. These models analyze historical data and real-time metrics to forecast potential security incidents.</p><p><u>Quantum-Safe Machine Learning</u></p><p>As quantum computing poses a threat to current encryption methods, there is ongoing research in making machine learning algorithms quantum-resistant.</p><p><u>AI for Security Orchestration and Automation</u></p><p>AI is being integrated into Security Information and Event Management (SIEM) systems to automate responses to security incidents, thereby reducing the time between detection and remediation.</p><p><u>Natural Language Processing for Phishing Detection</u></p><p>Advancements in NLP are enabling more sophisticated text analysis tools that can detect phishing attempts or fraudulent communications more effectively.</p><p><strong>How are these tools being used to protect decentralized networks?</strong></p><p><u>Federated Learning</u></p><p>What It Is: Federated learning is a decentralized machine learning approach where a model is trained across multiple nodes or devices without exchanging them. This ensures that sensitive data never leaves its original location.</p><p>Applications in Decentralized Networks: It's highly compatible with blockchain technology for decentralized data marketplaces or collaborative research across various organizations without compromising data privacy.</p><p>Regulatory Compliance: Since data doesn't leave its origin, federated learning can help organizations comply with data protection regulations like GDPR.</p><p><u>Differential Privacy</u></p><p>What It Is: Differential privacy adds noise to queries on a dataset, allowing for accurate analytics and machine learning without revealing any individual data point.</p><p>Applications in Decentralized Networks: Differential privacy can be applied to blockchain transactions to allow analytics or audits without compromising user privacy. This is critical in public blockchain networks where all transactions are transparent.</p><p>Regulatory Compliance: By ensuring that individual data is not identifiable, differential privacy helps in meeting privacy standards set by regulations like GDPR.</p><p><u>Homomorphic Encryption</u></p><p>What It Is: Homomorphic encryption allows for computations to be carried out on encrypted data. The results, when decrypted, are the same as if the operations had been performed on the raw data.</p><p>Applications in Decentralized Networks: In blockchain networks, homomorphic encryption can allow for confidential transactions and smart contracts that can be executed without revealing the underlying data to other network participants.</p><p>Regulatory Compliance: Since data remains encrypted even during computation, this technology can help in compliance with data protection laws, offering an extra layer of security.</p><p><u>Ethical Considerations</u></p><p>Data Minimization: These technologies align well with the ethical principle of data minimization, which is also a cornerstone of privacy laws like GDPR.</p><p>User Consent: Even with these technologies, it's crucial to obtain explicit user consent for data usage, particularly in decentralized settings where the emphasis on user control is higher.</p><p><strong>Challenges and limitations of implementing AI in decentralized security, including scalability, resource constraints, and potential biases?</strong></p><p><u></u></p>Scalability<p>High Computational Requirementced AI algorithms, particularly deep learning models, require significant computational power, which may not be feasible for all nodes in a decentralized network.s: Advan</p><p>Data Inconsistency: Decentralized networks often have varying data formats and structures, making it challenging to scale AI algorithms that require uniform data.</p><p><u>Resource Constraints</u></p><p>Energy Consumption: Both blockchain and resource-intensive AI algorithms consume substantial energy, raising environmental and cost concerns.</p><p>Hardware Limitations: Nodes in a decentralized network may have disparate hardware capabilities, making it difficult to implement AI solutions that require high computational resources uniformly.</p><p><u>Potential Biases</u></p><p>Data Bias: If the data used to train AI models reflects existing biases, the AI system itself can perpetuate or even exacerbate those biases, which is highly problematic in security applications.</p><p>Interpretability and Accountability: AI algorithms, especially complex ones like neural networks, are often considered 'black boxes' making it hard to interpret their decisions. This lack of transparency can be a significant issue, especially in decentralized systems where trust is crucial.</p><p><u>Ethical and Regulatory Concerns</u></p><p>Data Privacy: Even though AI can enhance security, its data collection and processing capabilities might infringe on user privacy if not carefully managed.</p><p>Centralization Risk: Implementing AI solutions often requires a level of centralization for data aggregation and model training, which could undermine the decentralized nature of the network.</p><p><u></u></p>Interoperability<p>System Compatibility: EnsuriAI algorithms are compatible with various blockchain protocols and smart contract languages can be challenging.ng that </p><p>Standardization: The lack of standardized frameworks for implementing AI in decentralized systems can hinder the seamless integration of these technologies.</p><p><u>Temporal Factors</u></p><p>Real-Time Analysis: Decentralized systems often require real-time security measures, but some AI algorithms may not be capable of providing real-time analysis due to computational constraints.</p><p>Model Drift: The behavior of decentralized networks can change over time, requiring AI models to be regularly updated, which can be challenging in a decentralized setting.</p>"
  },
  {
    "id": "cecl_regulatory_compliance_made_easy",
    "hidden": true,
    "image": "pexels-success.jpg",
    "title": "CECL Regulatory Compliance Made Easy",
    "categories": "CECL",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "<p><a href='/about'>Dr. Christos A. Makridis</a> is the founder and CEO of Dainamic and holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, and a senior adviser at Gallup.</p>",
    "link": "#",
    "published": "",
    "excerpt": "Most data models are in poor shape, making compliance hard. Dainamic's models can make it easy.",
    "text": "<p>The Current and Expected Credit Losses (CECL) standard was introduced by the Financial Accounting Standards Board a few years ago, and all banks must transition by December 15, 2022. CECL requires &quot;reasonable and supportable forecasts,&quot; which requires not only historical loan data, but also external data and a model that can be used to adjust for changes in economic conditions and a range of uncertainty over each scenario.</p><p>Sadly, most of the available models are dreadful and banks resort to historical averages of loan losses and aggregate conditions. This makes complying with CECL challenging for small and medium sized financial institutions for a number of reasons.</p><p><strong>1. Data consolidation takes time </strong></p><p>There is far too much data available for any single person to track, and there are no real platforms that quickly aggregate this data for an individual to use for better decisions.</p><p><strong>2. Signals vs noise</strong></p><p>It is difficult to get the right market signals from the data. Once you have collected the data it will take even more time to organize it and make sense of it all for actionable intelligence for decision-making.</p><p><strong>3. The pace of change renders even recent forecasts obsolete </strong></p><p>Forecasts from two months ago are not reliable today, especially in an era with substantial economic, social, and geopolitical uncertainty.</p><p><strong>4. Quantitative skills are expensive</strong></p><p>Hiring a competent team of data scientists is expensive and generally outside of the budget for mid/small sized banks. And because the new regulations are increasing the demand for data scientists, an area of expertise that is very in-demand in other industries, it will make it all the more difficult for small firms to build such teams. </p><p><strong>Dainamic can help </strong></p><p>The goal of Dainamic is to help banks with these key issues. First, we are a well qualified team of data scientists and economists who can take on the difficult task of data collection. We use the most sophisticated machine learning methods available with granular and comprehensive data over time to produce accurate and interpretable forecasts.</p><p>We also have the ability to analyze and visualize the data which will make it much easier for financial institutions to report it to regulators and other key stakeholders. This ultimately helps banks to make more reliable forecasts which will result in less stringent capital requirements and will help them deploy capital in more strategic ways.</p><p>To put it simply, better data drives better models. Better models drive better forecasts. And better forecasts drive better planning so banks can invest more intelligently. That is the goal of Dainamic. </p> <p>If you would like to learn more or would like our help, please contact us.</p>"
  },
  {
    "id": "modeling_capital_reserves_under_cecl",
    "hidden": true,
    "image": "pexels-financial-model.jpg",
    "title": "Modeling capital reserves under CECL",
    "categories": "CECL",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "<p><a href='/about'>Dr. Christos A. Makridis</a> is the founder and CEO of Dainamic and holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, and a senior adviser at Gallup.</p>",
    "published": "",
    "excerpt": "Once you have a reasonable and supportable forecast, how do you determine the amount of capital reserves that you actually need?",
    "text": "<p>The current expected credit losses methodology (CECL) for estimating allowances for credit losses requires that banks develop reasonable and supportable forecasts, which we&apos;ve written about before here. But once you have a reasonable and supportable forecast, how do you actually determine the amount of capital reserves that you need?</p><p>Conceptually, when forecasts suggest a downturn, a bank needs to have more reserves on hand. The rationale is simple: in a downturn, consumers and businesses are more likely to be delinquent on their loans, which not only reduces cash flow for a bank, but also increases the odds of foreclosure on the property, thereby affecting the value of their balance sheet and financial stability. In this sense, having more reserves on hand provides an extra cushion in the event of a crisis.</p><p>But holding reserves is costly. Sticking cash under a mattress has an opportunity cost â€“ banks cannot loan that money out to provide investment and consumption opportunities to consumers and businesses, in turn generating revenue on the interest on those loans. </p><p>We need a gold standard approach to determining reserves, striking the right balance between mitigating any risk of bank insolvency and providing value-generating opportunities.</p><p>We believe that modeling capital reserves is not rocket science â€“ and frankly there&apos;s probably more art than science in the modeling exercise since economics is fundamentally about behavior. Fancy models are helpful, but they cannot eliminate uncertainty all together. Here&apos;s our approach:</p><ol><li>Generate bank or region specific forecasts â€“ Depending on the size of the bank, you may simply need to know what&apos;s happening to a state or metropolitan area, but the key is that you obtain local predictions, rather than national ones. Even during the financial crisis of 2007-08, there were still some counties that were doing well, so especially for small and mid sized banks that do not have national exposure, there is no sense in using national forecasts.</li><li>Obtain a confidence interval on the forecast â€“ The quality and availability of data allows us to produce forecasts that have some degree of precision. Then, you want to choose how much precision you are willing to accept â€“ for example, a 99% confidence interval might be more than enough to rule out negative outcomes, particularly in light with other risks that the bank might take on a more regular basis.</li><li>Toggle the amount of reserves by asset class to achieve the desired level of safety â€“ Since each asset class is differentially exposed to risk because of either the composition of buyers in the market or other complementary asset classes it is linked to, banks can play around with different choices of reserves across the asset classes they are invested in to achieve their desired level of safety and security.</li></ol><p>These are not easy to do overnight for an organization, but we have made the tools available on our platform so that you do not need to go through the painstaking process of building it from scratch.</p><p>Learn more about the Dainamic Dashboard.</p>"
  },
  {
    "id": "strengthening_loss_forecasting_in_banking",
    "hidden": true,
    "image": "pexels-weightlifting.jpg",
    "title": "Strengthening Loss Forecasting in Banking",
    "categories": "CECL",
    "author": "Christos A. Makridis",
    "authorImage": "christos_nb.png",
    "role": "Founder & CEO, Dainamic",
    "bio": "<p><a href='/about'>Dr. Christos A. Makridis</a> is the founder and CEO of Dainamic and holds academic appointments at the Digital Economy Lab at Stanford University, the Chazen Institute at Columbia Business School, Institute for the Future at University of Nicosia, the Global Security Initiative and W. P. Carey School of Business at Arizona State University, and a senior adviser at Gallup.</p>",
    "published": "",
    "excerpt": "By anticipating potential credit losses, banks can better allocate capital and resources, minimizing the impact of financial stress on their operations.",
    "text": "<p>Accurate loss forecasting is crucial for banks to ensure financial stability and maintain regulatory compliance, particularly in the era of the Current Expected Credit Loss (CECL) accounting standard. By anticipating potential credit losses, banks can better allocate capital and resources, minimizing the impact of financial stress on their operations.</p><p>Here we will examine how banks can refine their credit loss estimation models to improve loss forecastings, risk management, and financial performance.</p><p><strong>1. Start with data collection and management</strong></p><p>If you want a good model, it all starts with good data and if you want a truly robust credit loss estimation model you will need access to  high-quality financial and market data. This would include loan-level data, like borrower characteristics, loan terms, and payment history. Additionally, banks should incorporate relevant macroeconomic variables, such as GDP growth, unemployment rates, and interest rates, which may affect credit losses.</p><p>Proper data management is also essential for maintaining data accuracy and consistency. Banks should develop a data governance framework that clearly defines roles, responsibilities, and processes for data collection, validation, storage, and updating. Implementing such a framework can help banks maintain data integrity and ensure that their models are based on reliable inputs.</p><p><strong>2. Selection of Appropriate Modeling Techniques for Credit Loss Estimation</strong></p><p>There is no one-size-fits-all approach to credit loss estimation, and banks should carefully consider the most suitable modeling techniques for their specific portfolio and risk profile.</p><p>Commonly used techniques include:</p><ul class='blog-list'><li>Statistical models, such as linear regression, logistic regression, or survival analysis, which can estimate credit losses based on historical data and relationships between variables.</li><li>Machine learning techniques, such as random forests, neural networks, or gradient boosting machines, which can identify complex patterns and non-linear relationships in data.</li><li>Expert judgment, in which subject matter experts provide input on model assumptions, parameter estimates, or qualitative adjustments based on their experience and knowledge.</li></ul><p>Banks should compare the performance of different modeling techniques and select the most appropriate method, or a combination of methods, that best captures the dynamics of their portfolio.</p><p><strong>3. Regular Model Validation and Calibration for Accurate Loss Forecasting</strong></p><p>Regular model validation is essential for ensuring the accuracy and reliability of credit loss estimation models. Banks should establish a systematic model validation process that includes:</p><ul class='blog-list'><li>Back-testing: Comparing model predictions with actual outcomes to assess the model's predictive accuracy.</li><li>Sensitivity analysis: Assessing the model's performance under different assumptions or input values to gauge its stability.</li><li>Benchmarking: Comparing the model's performance with alternative models or industry standards.</li></ul><p>Based on the validation results, banks should regularly calibrate their models, adjusting parameters or assumptions to improve accuracy and reflect changes in the economic environment, portfolio composition, or regulatory requirements.</p><p><strong>4. Incorporating Forward-Looking Scenarios in Credit Loss Forecasting</strong></p><p>CECL guidelines emphasize the importance of considering future economic conditions when estimating credit losses. Banks should incorporate forward-looking scenarios in their models, reflecting a range of plausible outcomes, such as baseline, adverse, and severely adverse economic scenarios. Scenario analysis can help banks better understand how their credit losses may evolve under different economic conditions and inform their capital planning and risk management strategies.</p><p><strong>5. Integrating Loss Forecasting with Risk Management Practices for Effective Banking Operations</strong></p><p>Loss forecasting should not be an isolated exercise but rather an integral part of a bank's risk management framework. Banks should align their loss forecasting processes with other risk management practices, such as stress testing, capital planning, and portfolio management. By integrating loss forecasting into their risk management systems, banks can more effectively monitor and manage credit risk and make informed strategic decisions.</p><p><strong>Final thoughts</strong></p><p>Enhancing loss forecasting capabilities is critical for banks to navigate the complex and evolving financial landscape. By focusing on data collection and management, selecting appropriate modeling techniques, regularly validating and calibrating their models, incorporating forward-looking scenarios, and integrating loss forecasting with risk management practices, banks can improve their credit loss estimation models and better prepare for potential</p>"
  }
]
